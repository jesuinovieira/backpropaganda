{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c19954cc",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import rsatoolbox.data\n",
    "import rsatoolbox.rdm.calc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import umap.umap_ as umap\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.models.feature_extraction\n",
    "from sklearn import manifold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import utils  # noqa: E402\n",
    "import models  # noqa: E402\n",
    "\n",
    "\n",
    "def sample_images(dataloader, n=5, plot=False):\n",
    "    \"\"\"Samples a specified number of images from a data loader.\"\"\"\n",
    "    imgs, labels = next(iter(dataloader))\n",
    "\n",
    "    imgs_o = []\n",
    "    targets = []\n",
    "    for value in range(10):\n",
    "        class_imgs = imgs[labels == value]\n",
    "        num_to_take = min(n, len(class_imgs))\n",
    "        imgs_o.append(class_imgs[:num_to_take])\n",
    "        targets.extend([value] * num_to_take)\n",
    "\n",
    "    imgs = torch.cat(imgs_o, dim=0)\n",
    "    targets = torch.tensor(targets).flatten()\n",
    "\n",
    "    if plot:\n",
    "        plt.imshow(\n",
    "            torch.moveaxis(\n",
    "                make_grid(imgs, nrow=n, padding=0, normalize=False, pad_value=0), 0, -1\n",
    "            )\n",
    "        )\n",
    "        plt.title(\"Sampled Test Images (5 of each class)\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    return imgs, targets\n",
    "\n",
    "\n",
    "def extract_features(model, imgs, return_layers):\n",
    "    \"\"\"Extracts features from specified layers of the model.\"\"\"\n",
    "    if return_layers == \"all\":\n",
    "        # Automatically get the names of all layers in the model\n",
    "        return_layers, _ = torchvision.models.feature_extraction.get_graph_node_names(\n",
    "            model\n",
    "        )\n",
    "\n",
    "    # Create the feature extractor\n",
    "    feature_extractor = torchvision.models.feature_extraction.create_feature_extractor(\n",
    "        model, return_nodes=return_layers\n",
    "    )\n",
    "    model_features = feature_extractor(imgs)\n",
    "\n",
    "    # Add input images (not a layer, but useful for RDM comparison)\n",
    "    model_features = {\"input\": imgs, **model_features}\n",
    "    return model_features\n",
    "\n",
    "\n",
    "def calc_rdms(model_features, method=\"correlation\"):\n",
    "    \"\"\"Calculates representational dissimilarity matrices (RDMs) for model features.\n",
    "\n",
    "    Args:\n",
    "        model_features: A dictionary where keys are layer names and values are features of the layers.\n",
    "        method: The method to calculate RDMs, e.g., 'correlation'. Default is 'correlation'.\n",
    "\n",
    "    Outputs:\n",
    "        rdms: RDMs object containing dissimilarity matrices.\n",
    "        rdms_dict: A dictionary with layer names as keys and their corresponding RDMs as values.\n",
    "    \"\"\"\n",
    "    ds_list = []\n",
    "    for l in range(len(model_features)):\n",
    "        layer = list(model_features.keys())[l]\n",
    "        feats = model_features[layer]\n",
    "\n",
    "        if type(feats) is list:\n",
    "            feats = feats[-1]\n",
    "\n",
    "        feats = feats.cpu()\n",
    "\n",
    "        if len(feats.shape) > 2:\n",
    "            feats = feats.flatten(1)\n",
    "\n",
    "        feats = feats.detach().numpy()\n",
    "        ds = rsatoolbox.data.Dataset(feats, descriptors=dict(layer=layer))\n",
    "        ds_list.append(ds)\n",
    "\n",
    "    rdms = rsatoolbox.rdm.calc.calc_rdm(ds_list, method=method)\n",
    "    rdms_dict = {\n",
    "        list(model_features.keys())[i]: rdms.get_matrices()[i]\n",
    "        for i in range(len(model_features))\n",
    "    }\n",
    "\n",
    "    return rdms, rdms_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd9e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "n_classes = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load models\n",
    "bp_model = models.LeNet5(n_classes=10, latent_dim=84, act_fn=\"relu\")\n",
    "bp_model.load_state_dict(torch.load(\"../results/backprop-model.pth\"))\n",
    "bp_model.eval().to(device)\n",
    "\n",
    "ff_model = models.FFLeNet5(n_classes=10, latent_dim=84)\n",
    "ff_model.load_state_dict(torch.load(\"../results/ff-model.pth\"))\n",
    "ff_model.eval().to(device)\n",
    "\n",
    "pc_model = models.PCLeNet5(n_classes=10, latent_dim=84)\n",
    "pc_model.load_state_dict(torch.load(\"../results/pc-model.pth\"))\n",
    "pc_model.eval().to(device)\n",
    "\n",
    "# Load data\n",
    "batch_size = 50000  # Does not matter here\n",
    "_, _, test_loader = utils.load_mnist_data(batch_size)\n",
    "class_names = [str(i) for i in range(n_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d462f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes, val_nodes = get_graph_node_names(bp_model)\n",
    "print(f\"LeNet5: {train_nodes=}\")\n",
    "train_nodes, val_nodes = get_graph_node_names(ff_model)\n",
    "print(f\"FFLeNet5: {train_nodes=}\")\n",
    "train_nodes, val_nodes = get_graph_node_names(pc_model)\n",
    "print(f\"PCLeNet5: {train_nodes=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import train.ff\n",
    "\n",
    "\n",
    "def get_features(dataloader, n):\n",
    "    imgs, targets = sample_images(dataloader, n=n, plot=False)\n",
    "    n = imgs.shape[0]\n",
    "\n",
    "    # Move images to device\n",
    "    imgs, targets = imgs.to(device), targets.to(device)\n",
    "    print(f\"Sampled {imgs.shape[0]} images with shape {imgs.shape[1:]}\")\n",
    "    imgs_overlay = train.ff.overlay_label(imgs, targets, n_classes, is_positive=True)\n",
    "    imgs_overlay = imgs_overlay.to(device)\n",
    "\n",
    "    # Extract features from all models\n",
    "    print(\"Extracting features (backprop)\")\n",
    "    layer_names = [\"pool1\", \"pool2\", \"relu_2\", \"relu_3\", \"fc2\"]\n",
    "    bp_features = extract_features(bp_model, imgs, return_layers=layer_names)\n",
    "\n",
    "    print(\"Extracting features (forward-forward)\")\n",
    "    layer_names = [\"pool1\", \"pool2\", \"conv3.relu\", \"fc1.relu\", \"fc2.linear\"]\n",
    "    ff_features = extract_features(ff_model, imgs_overlay, return_layers=layer_names)\n",
    "\n",
    "    print(\"Extracting features (predictive coding)\")\n",
    "    layer_names = [\"0.2\", \"1.2\", \"2.1\", \"3.2\", \"4.0\"]\n",
    "    pc_features = extract_features(pc_model, imgs, return_layers=layer_names)\n",
    "\n",
    "    # Rename to match the backpropagation model\n",
    "    ff_features = {\n",
    "        \"input\": ff_features[\"input\"],\n",
    "        \"pool1\": ff_features[\"pool1\"],\n",
    "        \"pool2\": ff_features[\"pool2\"],\n",
    "        \"relu_2\": ff_features[\"conv3.relu\"],\n",
    "        \"relu_3\": ff_features[\"fc1.relu\"],\n",
    "        \"fc2\": ff_features[\"fc2.linear\"],\n",
    "    }\n",
    "\n",
    "    pc_features = {\n",
    "        \"input\": pc_features[\"input\"],\n",
    "        \"pool1\": pc_features[\"0.2\"],\n",
    "        \"pool2\": pc_features[\"1.2\"],\n",
    "        \"relu_2\": pc_features[\"2.1\"],\n",
    "        \"relu_3\": pc_features[\"3.2\"],\n",
    "        \"fc2\": pc_features[\"4.0\"],\n",
    "    }\n",
    "\n",
    "    return imgs, imgs_overlay, targets, bp_features, ff_features, pc_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b8963f",
   "metadata": {},
   "source": [
    "## RDMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5018d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_maps(model_features, model_name):\n",
    "    \"\"\"Plots representational dissimilarity matrices (RDMs) across different layers.\"\"\"\n",
    "    fig = plt.figure(figsize=(14, 4))\n",
    "    fig.suptitle(\n",
    "        f\"RDMs Across Layers – {model_name}\", fontsize=16, weight=\"bold\", y=0.88\n",
    "    )\n",
    "    gs = fig.add_gridspec(1, len(model_features))\n",
    "    fig.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "\n",
    "    for l, layer in enumerate(model_features.keys()):\n",
    "        map_ = np.squeeze(model_features[layer])\n",
    "\n",
    "        if len(map_.shape) < 2:\n",
    "            side_len = int(np.sqrt(map_.shape[0]))\n",
    "            if side_len * side_len == map_.shape[0]:\n",
    "                map_ = map_.reshape((side_len, side_len))\n",
    "\n",
    "        if np.max(map_) > 0:\n",
    "            map_ = map_ / np.max(map_)\n",
    "\n",
    "        ax = plt.subplot(gs[0, l])\n",
    "        ax_ = ax.imshow(map_, cmap=\"magma_r\")\n",
    "        ax.set_title(f\"{layer}\")\n",
    "        ax.set_xlabel(\"Input Index\")\n",
    "        if l == 0:\n",
    "            ax.set_ylabel(\"Input Index\")\n",
    "\n",
    "    fig.subplots_adjust(right=0.9)\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n",
    "    # cbar_ax = fig.add_axes([0.92, 0.3, 0.015, 0.4])\n",
    "    cbar = fig.colorbar(ax_, cax=cbar_ax)\n",
    "    cbar.set_label(\"Dissimilarity\", rotation=270, labelpad=15)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69481f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, imgs_overlay, targets, bp_features, ff_features, pc_features = get_features(\n",
    "    test_loader, n=5\n",
    ")\n",
    "\n",
    "# TODO: single plot for all models\n",
    "rdms_bp, rdms_dict_bp = calc_rdms(\n",
    "    {k: v for k, v in bp_features.items() if k != \"input\"}\n",
    ")\n",
    "plot_maps(rdms_dict_bp, \"BP\")\n",
    "\n",
    "rdms_ff, rdms_dict_ff = calc_rdms(\n",
    "    {k: v for k, v in ff_features.items() if k != \"input\"}\n",
    ")\n",
    "plot_maps(rdms_dict_ff, \"FF\")\n",
    "\n",
    "rdms_pc, rdms_dict_pc = calc_rdms(\n",
    "    {k: v for k, v in pc_features.items() if k != \"input\"}\n",
    ")\n",
    "plot_maps(rdms_dict_pc, \"PC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a30f7f6",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e1c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dim_reduction(model_features, labels, transformer_funcs, algorithm):\n",
    "    \"\"\"Plots the dimensionality reduction results for model features.\"\"\"\n",
    "    transformers = []\n",
    "    for t in transformer_funcs:\n",
    "        if t == \"PCA\":\n",
    "            transformers.append(PCA(n_components=2, random_state=42))\n",
    "        elif t == \"MDS\":\n",
    "            transformers.append(\n",
    "                manifold.MDS(n_components=2, normalized_stress=\"auto\", random_state=42)\n",
    "            )\n",
    "        elif t == \"t-SNE\":\n",
    "            transformers.append(\n",
    "                manifold.TSNE(n_components=2, perplexity=40, verbose=0, random_state=42)\n",
    "            )\n",
    "        elif t == \"UMAP\":\n",
    "            transformers.append(umap.UMAP(n_components=2, random_state=42))\n",
    "\n",
    "    n_rows = len(transformers)\n",
    "    n_cols = len(model_features)\n",
    "    fig = plt.figure(figsize=(2.25 * n_cols, 2.25 * n_rows))\n",
    "    gs = fig.add_gridspec(n_rows, n_cols)\n",
    "    fig.subplots_adjust(wspace=0.25, hspace=0.25)\n",
    "\n",
    "    return_layers = list(model_features.keys())\n",
    "\n",
    "    num_classes = len(np.unique(labels))\n",
    "    cmap = plt.get_cmap(\"viridis_r\", num_classes)\n",
    "    norm = mpl.colors.BoundaryNorm(np.arange(-0.5, num_classes), cmap.N)\n",
    "\n",
    "    for i, transformer in enumerate(transformers):\n",
    "        for j, layer in enumerate(return_layers):\n",
    "            feats = model_features[layer].detach().cpu().flatten(1)\n",
    "            feats_transformed = transformer.fit_transform(feats)\n",
    "\n",
    "            ax = fig.add_subplot(gs[i, j])\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "            x_min, x_max = feats_transformed[:, 0].min(), feats_transformed[:, 0].max()\n",
    "            y_min, y_max = feats_transformed[:, 1].min(), feats_transformed[:, 1].max()\n",
    "            x_center, y_center = (x_min + x_max) / 2, (y_min + y_max) / 2\n",
    "            max_range = max(x_max - x_min, y_max - y_min) * 0.65\n",
    "            ax.set_xlim(x_center - max_range, x_center + max_range)\n",
    "            ax.set_ylim(y_center - max_range, y_center + max_range)\n",
    "\n",
    "            scatter = ax.scatter(\n",
    "                feats_transformed[:, 0],\n",
    "                feats_transformed[:, 1],\n",
    "                c=labels,\n",
    "                cmap=cmap,\n",
    "                norm=norm,\n",
    "                s=25,\n",
    "                edgecolors=\"white\",\n",
    "                linewidths=0.5,\n",
    "                alpha=0.9,\n",
    "            )\n",
    "\n",
    "            if i == 0:\n",
    "                ax.set_title(layer, fontsize=14)\n",
    "            if j == 0:\n",
    "                ax.annotate(\n",
    "                    transformer_funcs[i],\n",
    "                    xy=(-0.25, 0.5),\n",
    "                    xycoords=\"axes fraction\",\n",
    "                    ha=\"right\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=14,\n",
    "                    rotation=90,\n",
    "                )\n",
    "\n",
    "    cbar_ax = fig.add_axes([0.92, 0.2, 0.015, 0.6])\n",
    "    cbar = fig.colorbar(scatter, cax=cbar_ax, ticks=np.arange(num_classes))\n",
    "    cbar.ax.set_ylabel(\"Class\", rotation=270, labelpad=15)\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"Dimensionality Reduction Across Layers – {algorithm}\",\n",
    "        fontsize=16,\n",
    "        weight=\"bold\",\n",
    "        x=0.53,\n",
    "        y=1.0,\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_dim_reduction_multiple_algorithms(\n",
    "    all_model_features: dict, labels, transformer_name: str\n",
    "):\n",
    "    # Select transformer\n",
    "    if transformer_name == \"PCA\":\n",
    "        transformer = PCA(n_components=2, random_state=42)\n",
    "    elif transformer_name == \"MDS\":\n",
    "        transformer = manifold.MDS(\n",
    "            n_components=2, normalized_stress=\"auto\", random_state=42\n",
    "        )\n",
    "    elif transformer_name == \"t-SNE\":\n",
    "        transformer = manifold.TSNE(\n",
    "            n_components=2, perplexity=40, verbose=0, random_state=42\n",
    "        )\n",
    "    elif transformer_name == \"UMAP\":\n",
    "        transformer = umap.UMAP(n_components=2, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown transformer: {transformer_name}\")\n",
    "\n",
    "    algo_names = list(all_model_features.keys())\n",
    "    layers = list(next(iter(all_model_features.values())).keys())\n",
    "\n",
    "    n_rows = len(algo_names)\n",
    "    n_cols = len(layers)\n",
    "    fig = plt.figure(figsize=(2.25 * n_cols, 2.25 * n_rows))\n",
    "    gs = fig.add_gridspec(n_rows, n_cols)\n",
    "    fig.subplots_adjust(wspace=0.25, hspace=0.25)\n",
    "\n",
    "    num_classes = len(np.unique(labels))\n",
    "    cmap = plt.get_cmap(\"viridis_r\", num_classes)\n",
    "    norm = mpl.colors.BoundaryNorm(np.arange(-0.5, num_classes), cmap.N)\n",
    "\n",
    "    for i, algo in enumerate(algo_names):\n",
    "        for j, layer in enumerate(layers):\n",
    "            feats = all_model_features[algo][layer].detach().cpu().flatten(1)\n",
    "            feats_transformed = transformer.fit_transform(feats)\n",
    "\n",
    "            ax = fig.add_subplot(gs[i, j])\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "            # Centered axis limits\n",
    "            x_min, x_max = feats_transformed[:, 0].min(), feats_transformed[:, 0].max()\n",
    "            y_min, y_max = feats_transformed[:, 1].min(), feats_transformed[:, 1].max()\n",
    "            x_center, y_center = (x_min + x_max) / 2, (y_min + y_max) / 2\n",
    "            max_range = max(x_max - x_min, y_max - y_min) * 0.65\n",
    "            ax.set_xlim(x_center - max_range, x_center + max_range)\n",
    "            ax.set_ylim(y_center - max_range, y_center + max_range)\n",
    "\n",
    "            scatter = ax.scatter(\n",
    "                feats_transformed[:, 0],\n",
    "                feats_transformed[:, 1],\n",
    "                c=labels,\n",
    "                cmap=cmap,\n",
    "                norm=norm,\n",
    "                s=25,\n",
    "                edgecolors=\"white\",\n",
    "                linewidths=0.5,\n",
    "                alpha=0.9,\n",
    "            )\n",
    "\n",
    "            if i == 0:\n",
    "                ax.set_title(layer, fontsize=14)\n",
    "            if j == 0:\n",
    "                ax.annotate(\n",
    "                    algo,\n",
    "                    xy=(-0.25, 0.5),\n",
    "                    xycoords=\"axes fraction\",\n",
    "                    ha=\"right\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=14,\n",
    "                    rotation=90,\n",
    "                )\n",
    "\n",
    "    # Colorbar\n",
    "    cbar_ax = fig.add_axes([0.92, 0.2, 0.015, 0.6])\n",
    "    cbar = fig.colorbar(scatter, cax=cbar_ax, ticks=np.arange(num_classes))\n",
    "    cbar.ax.set_ylabel(\"Class\", rotation=270, labelpad=15)\n",
    "\n",
    "    # Auto-centering suptitle\n",
    "    title_x = (fig.subplotpars.left + fig.subplotpars.right) / 2\n",
    "    fig.suptitle(\n",
    "        f\"Dimensionality Reduction Across Layers\",\n",
    "        fontsize=16,\n",
    "        weight=\"bold\",\n",
    "        x=title_x,\n",
    "        y=1.0,\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cbd401",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dim_reduction(\n",
    "    bp_features,\n",
    "    targets,\n",
    "    transformer_funcs=[\"PCA\", \"t-SNE\", \"UMAP\"],\n",
    "    algorithm=\"Backprop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc75655",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dim_reduction(\n",
    "    ff_features,\n",
    "    targets,\n",
    "    transformer_funcs=[\"PCA\", \"t-SNE\", \"UMAP\"],\n",
    "    algorithm=\"Forward-forward\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c086be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dim_reduction(\n",
    "    pc_features,\n",
    "    targets,\n",
    "    transformer_funcs=[\"PCA\", \"t-SNE\", \"MDS\", \"UMAP\"],\n",
    "    algorithm=\"Predictive Coding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc68772",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dim_reduction_multiple_algorithms(\n",
    "    {\"backprop\": bp_features, \"ff\": ff_features, \"pc\": pc_features},\n",
    "    targets,\n",
    "    transformer_name=\"UMAP\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104df37b",
   "metadata": {},
   "source": [
    "## Representational Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df64da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def rep_path(\n",
    "    model_features,\n",
    "    model_colors,\n",
    "    labels=None,\n",
    "    rdm_calc_method=\"euclidean\",\n",
    "    rdm_comp_method=\"cosine\",\n",
    "):\n",
    "    \"\"\"Represents paths of model features in a reduced-dimensional space.\n",
    "\n",
    "    Inputs:\n",
    "    - model_features: Dictionary containing model features for each model.\n",
    "    - model_colors: Dictionary mapping model names to colors for visualization.\n",
    "    - labels: Array of labels corresponding to the model features.\n",
    "    - rdm_calc_method: Method for calculating RDMS ('euclidean' or 'correlation').\n",
    "    - rdm_comp_method: Method for comparing RDMS ('cosine' or 'corr').\n",
    "    \"\"\"\n",
    "    path_len = []\n",
    "    path_colors = []\n",
    "    rdms_list = []\n",
    "    ax_ticks = []\n",
    "    tick_colors = []\n",
    "    model_names = list(model_features.keys())\n",
    "\n",
    "    for m in range(len(model_names)):\n",
    "        model_name = model_names[m]\n",
    "        features = model_features[model_name]\n",
    "        path_colors.append(model_colors[model_name])\n",
    "        path_len.append(len(features))\n",
    "        ax_ticks.append(list(features.keys()))\n",
    "        tick_colors.append([model_colors[model_name]] * len(features))\n",
    "        rdms, _ = calc_rdms(features, method=rdm_calc_method)\n",
    "        rdms_list.append(rdms)\n",
    "\n",
    "    path_len = np.insert(np.cumsum(path_len), 0, 0)\n",
    "    input_idxs = list(path_len[:-1])\n",
    "\n",
    "    if labels is not None:\n",
    "        rdms, _ = calc_rdms(\n",
    "            {\"labels\": F.one_hot(labels).float().to(device)}, method=rdm_calc_method\n",
    "        )\n",
    "        rdms_list.append(rdms)\n",
    "        ax_ticks.append([\"labels\"])\n",
    "        tick_colors.append([\"m\"])\n",
    "        idx_labels = -1\n",
    "\n",
    "    rdms = rsatoolbox.rdm.concat(rdms_list)\n",
    "\n",
    "    # Flatten the list\n",
    "    ax_ticks = [l for model_layers in ax_ticks for l in model_layers]\n",
    "    tick_colors = [l for model_layers in tick_colors for l in model_layers]\n",
    "    tick_colors = [\n",
    "        \"k\" if tick == \"input\" else color for tick, color in zip(ax_ticks, tick_colors)\n",
    "    ]\n",
    "\n",
    "    rdms_comp = rsatoolbox.rdm.compare(rdms, rdms, method=rdm_comp_method)\n",
    "    if rdm_comp_method == \"cosine\":\n",
    "        rdms_comp = np.clip(rdms_comp, -1.0, 1.0)\n",
    "        rdms_comp = np.arccos(rdms_comp)\n",
    "    rdms_comp = np.nan_to_num(rdms_comp, nan=0.0)\n",
    "\n",
    "    # Symmetrize\n",
    "    rdms_comp = (rdms_comp + rdms_comp.T) / 2.0\n",
    "\n",
    "    # Reduce dim to 2\n",
    "    transformer = manifold.MDS(\n",
    "        n_components=2,\n",
    "        max_iter=1000,\n",
    "        n_init=10,\n",
    "        normalized_stress=\"auto\",\n",
    "        dissimilarity=\"precomputed\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    dims = transformer.fit_transform(rdms_comp)\n",
    "\n",
    "    # Remove duplicates of the input layer from multiple models\n",
    "    # remove_duplicates = np.where(np.array(ax_ticks) == 'input')[0][1:]\n",
    "    # for index in remove_duplicates:\n",
    "    #     del ax_ticks[index]\n",
    "    #     del tick_colors[index]\n",
    "    #     rdms_comp = np.delete(np.delete(rdms_comp, index, axis=0), index, axis=1)\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 4))\n",
    "    gs = fig.add_gridspec(1, 2, width_ratios=[1, 1.1])\n",
    "    fig.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "\n",
    "    # Plot dissimilarity matrix\n",
    "    ax = plt.subplot(gs[0, 0])\n",
    "    ax_ = ax.imshow(rdms_comp, cmap=\"viridis_r\")\n",
    "    fig.subplots_adjust(left=0.2)\n",
    "    cbar_ax = fig.add_axes([-0.01, 0.2, 0.01, 0.5])\n",
    "    fig.colorbar(ax_, cax=cbar_ax, location=\"left\")\n",
    "    ax.set_title(\"Dissimilarity Between Layer RDMs\", fontdict={\"fontsize\": 14}, y=1.02)\n",
    "    ax.set_xticks(np.arange(len(ax_ticks)), labels=ax_ticks, fontsize=7, rotation=90)\n",
    "    ax.set_yticks(np.arange(len(ax_ticks)), labels=ax_ticks, fontsize=7)\n",
    "    [t.set_color(i) for (i, t) in zip(tick_colors, ax.xaxis.get_ticklabels())]\n",
    "    [t.set_color(i) for (i, t) in zip(tick_colors, ax.yaxis.get_ticklabels())]\n",
    "\n",
    "    # Colorbar\n",
    "    ax = plt.subplot(gs[0, 1])\n",
    "    amin, amax = dims.min(), dims.max()\n",
    "    amin, amax = (amin + amax) / 2 - (amax - amin) * 5 / 8, (amin + amax) / 2 + (\n",
    "        amax - amin\n",
    "    ) * 5 / 8\n",
    "\n",
    "    # Plot geometry paths\n",
    "    for i in range(len(rdms_list) - 1):\n",
    "        path_indices = np.arange(path_len[i], path_len[i + 1])\n",
    "        ax.plot(\n",
    "            dims[path_indices, 0],\n",
    "            dims[path_indices, 1],\n",
    "            color=path_colors[i],\n",
    "            marker=\".\",\n",
    "            label=model_names[i],\n",
    "        )\n",
    "        ax.set_title(\n",
    "            \"Representational Geometry Path\", fontdict={\"fontsize\": 14}, y=1.02\n",
    "        )\n",
    "        ax.set_xlim([amin, amax])\n",
    "        ax.set_ylim([amin, amax])\n",
    "\n",
    "    for idx in input_idxs:\n",
    "        if idx == 0:\n",
    "            ax.plot(\n",
    "                dims[idx, 0],\n",
    "                dims[idx, 1],\n",
    "                color=\"k\",\n",
    "                marker=\"s\",\n",
    "                markersize=6,\n",
    "                label=\"input\",\n",
    "                linestyle=\"None\",\n",
    "            )\n",
    "        ax.plot(dims[idx, 0], dims[idx, 1], color=\"k\", marker=\"s\", markersize=6)\n",
    "\n",
    "    if labels is not None:\n",
    "        ax.plot(\n",
    "            dims[idx_labels, 0],\n",
    "            dims[idx_labels, 1],\n",
    "            color=\"m\",\n",
    "            marker=\"*\",\n",
    "            markersize=10,\n",
    "            label=\"label\",\n",
    "            linestyle=\"None\",\n",
    "        )\n",
    "\n",
    "    ax.legend(fontsize=8, loc=\"best\", frameon=False)\n",
    "    fig.suptitle(\n",
    "        \"Representational Geometry Comparison\", fontsize=15, weight=\"bold\", y=1.03\n",
    "    )\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d765a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import importlib\n",
    "\n",
    "importlib.reload(train.ff)\n",
    "\n",
    "imgs, imgs_overlay, targets, bp_features, ff_features, pc_features = get_features(\n",
    "    test_loader, n=750\n",
    ")\n",
    "\n",
    "all_features = {\n",
    "    \"BP\": bp_features,\n",
    "    \"FF\": {k: v for k, v in ff_features.items() if k != \"input\"},\n",
    "    \"PC\": {k: v for k, v in pc_features.items() if k != \"input\"},\n",
    "}\n",
    "\n",
    "colors = sns.color_palette(\"deep\", len(all_features))\n",
    "colors = {k: color for k, color in zip(all_features.keys(), colors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8fdc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How it moves from input (raw image) to output (label)\n",
    "# That’s why the pink star is static — it doesn't come from a layer, but from the target structure you're aiming for\n",
    "# The label point summarizes how you'd ideally want the model to group the data — based on all class labels, not just one\n",
    "#\n",
    "# The figure on the right shows the embedding of these distances in a 2D space using MDS\n",
    "# This figure captures the same distances between the layers shown in the matrix on the left in a 2D space\n",
    "\n",
    "# Conclusions\n",
    "#\n",
    "# Each learning rule (BP, FF, PC) transforms the input along a distinct representational path\n",
    "# BP ends closest to the label RDM — it optimizes for class separation explicitly\n",
    "# FF ends far from the label RDM, yet still achieves high accuracy — showing that alternative geometries can still support classification\n",
    "# A model doesn’t need to replicate the ideal class structure (label RDM) — just needs to encode discriminative features\n",
    "# This highlights that representation quality and classification performance aren’t always perfectly aligned\n",
    "#\n",
    "# FF doesn’t use softmax over a single output vector like backprop\n",
    "# Instead, it computes a “goodness” score for each class separately by running the input with the label embedded\n",
    "# The predicted class is the one with the highest goodness, not the highest softmax probability\n",
    "# Thus, FF encodes label information early, and inference is based on which label makes the network respond most positively — not a shared decision head.\n",
    "\n",
    "# TODO: modify the function to select from a single class if desired\n",
    "# TODO: use any batch size but respect n samples desired\n",
    "rep_path(all_features, colors, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a8f9fb",
   "metadata": {},
   "source": [
    "## Bootstrap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
