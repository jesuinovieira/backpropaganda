{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e2d535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import rsatoolbox.data\n",
    "import rsatoolbox.rdm.calc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import utils  # noqa: E402\n",
    "import models  # noqa: E402\n",
    "\n",
    "\n",
    "def sample_images(data_loader, n=5, plot=False):\n",
    "    \"\"\"Samples a specified number of images from a data loader.\"\"\"\n",
    "    imgs, labels = next(iter(data_loader))\n",
    "\n",
    "    imgs_o = []\n",
    "    targets = []\n",
    "    for value in range(10):\n",
    "        imgs_o.append(imgs[np.where(labels == value)][0:n])\n",
    "        targets.append([value] * n)\n",
    "\n",
    "    imgs = torch.cat(imgs_o, dim=0)\n",
    "    targets = torch.tensor(targets).flatten()\n",
    "\n",
    "    if plot:\n",
    "        plt.imshow(\n",
    "            torch.moveaxis(\n",
    "                make_grid(imgs, nrow=n, padding=0, normalize=False, pad_value=0), 0, -1\n",
    "            )\n",
    "        )\n",
    "        plt.title(\"Sampled Test Images (5 of each class)\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    return imgs, targets\n",
    "\n",
    "\n",
    "def extract_features(model, imgs, return_layers):\n",
    "    \"\"\"Extracts features from specified layers of the model.\"\"\"\n",
    "    if return_layers == \"all\":\n",
    "        # Automatically get the names of all layers in the model\n",
    "        return_layers, _ = get_graph_node_names(model)\n",
    "\n",
    "    # Create the feature extractor\n",
    "    feature_extractor = create_feature_extractor(model, return_nodes=return_layers)\n",
    "    model_features = feature_extractor(imgs)\n",
    "\n",
    "    # Add input images (not a layer, but useful for RDM comparison)\n",
    "    model_features = {\"input\": imgs, **model_features}\n",
    "    return model_features\n",
    "\n",
    "\n",
    "def calc_rdms(model_features, method=\"correlation\"):\n",
    "    \"\"\"Calculates representational dissimilarity matrices (RDMs) for model features.\n",
    "\n",
    "    Args:\n",
    "        model_features: A dictionary where keys are layer names and values are features of the layers.\n",
    "        method: The method to calculate RDMs, e.g., 'correlation'. Default is 'correlation'.\n",
    "\n",
    "    Outputs:\n",
    "        rdms: RDMs object containing dissimilarity matrices.\n",
    "        rdms_dict: A dictionary with layer names as keys and their corresponding RDMs as values.\n",
    "    \"\"\"\n",
    "    ds_list = []\n",
    "    for l in range(len(model_features)):\n",
    "        layer = list(model_features.keys())[l]\n",
    "        feats = model_features[layer]\n",
    "\n",
    "        if type(feats) is list:\n",
    "            feats = feats[-1]\n",
    "\n",
    "        feats = feats.cpu()\n",
    "\n",
    "        if len(feats.shape) > 2:\n",
    "            feats = feats.flatten(1)\n",
    "\n",
    "        feats = feats.detach().numpy()\n",
    "        ds = rsatoolbox.data.Dataset(feats, descriptors=dict(layer=layer))\n",
    "        ds_list.append(ds)\n",
    "\n",
    "    rdms = rsatoolbox.rdm.calc.calc_rdm(ds_list, method=method)\n",
    "    rdms_dict = {\n",
    "        list(model_features.keys())[i]: rdms.get_matrices()[i]\n",
    "        for i in range(len(model_features))\n",
    "    }\n",
    "\n",
    "    return rdms, rdms_dict\n",
    "\n",
    "\n",
    "def plot_maps(model_features, model_name):\n",
    "    \"\"\"Plots representational dissimilarity matrices (RDMs) across different layers.\"\"\"\n",
    "    fig = plt.figure(figsize=(14, 4))\n",
    "    fig.suptitle(f\"RDMs across layers â€“ {model_name}\")\n",
    "    gs = fig.add_gridspec(1, len(model_features))\n",
    "    fig.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "\n",
    "    for l, layer in enumerate(model_features.keys()):\n",
    "        map_ = np.squeeze(model_features[layer])\n",
    "\n",
    "        if len(map_.shape) < 2:\n",
    "            side_len = int(np.sqrt(map_.shape[0]))\n",
    "            if side_len * side_len == map_.shape[0]:\n",
    "                map_ = map_.reshape((side_len, side_len))\n",
    "\n",
    "        if np.max(map_) > 0:\n",
    "            map_ = map_ / np.max(map_)\n",
    "\n",
    "        ax = plt.subplot(gs[0, l])\n",
    "        ax_ = ax.imshow(map_, cmap=\"magma_r\")\n",
    "        ax.set_title(f\"Layer: {layer}\")\n",
    "        ax.set_xlabel(\"Input Index\")\n",
    "        if l == 0:\n",
    "            ax.set_ylabel(\"Input Index\")\n",
    "\n",
    "    fig.subplots_adjust(right=0.9)\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n",
    "    cbar = fig.colorbar(ax_, cax=cbar_ax)\n",
    "    cbar.set_label(\"Dissimilarity\", rotation=270, labelpad=15)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4fd9e98",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FFLeNet5:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"conv3.weight\", \"conv3.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"conv1.conv.weight\", \"conv1.conv.bias\", \"conv2.conv.weight\", \"conv2.conv.bias\", \"conv3.conv.weight\", \"conv3.conv.bias\", \"fc1.linear.weight\", \"fc1.linear.bias\", \"fc2.linear.weight\", \"fc2.linear.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m bp_model.eval().to(device)\n\u001b[32m     15\u001b[39m ff_model = models.FFLeNet5(n_classes=\u001b[32m10\u001b[39m, latent_dim=\u001b[32m84\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mff_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../results/ff-model.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m ff_model.eval().to(device)\n\u001b[32m     19\u001b[39m pc_model = models.PCLeNet5(n_classes=\u001b[32m10\u001b[39m, latent_dim=\u001b[32m84\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/backpropaganda/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for FFLeNet5:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"conv3.weight\", \"conv3.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"conv1.conv.weight\", \"conv1.conv.bias\", \"conv2.conv.weight\", \"conv2.conv.bias\", \"conv3.conv.weight\", \"conv3.conv.bias\", \"fc1.linear.weight\", \"fc1.linear.bias\", \"fc2.linear.weight\", \"fc2.linear.bias\". "
     ]
    }
   ],
   "source": [
    "# Config\n",
    "n_classes = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load data\n",
    "batch_size = 500  # Does not matter here\n",
    "_, _, test_loader = utils.load_mnist_data(batch_size)\n",
    "class_names = [str(i) for i in range(n_classes)]\n",
    "\n",
    "# Load models\n",
    "bp_model = models.LeNet5(n_classes=10, latent_dim=84, act_fn=\"relu\")\n",
    "bp_model.load_state_dict(torch.load(\"../results/backprop-model.pth\"))\n",
    "bp_model.eval().to(device)\n",
    "\n",
    "ff_model = models.FFLeNet5(n_classes=10, latent_dim=84)\n",
    "ff_model.load_state_dict(torch.load(\"../results/ff-model.pth\"))\n",
    "ff_model.eval().to(device)\n",
    "\n",
    "pc_model = models.PCLeNet5(n_classes=10, latent_dim=84)\n",
    "pc_model.load_state_dict(torch.load(\"../results/pc-model.pth\"))\n",
    "pc_model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d462f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes, val_nodes = get_graph_node_names(bp_model)\n",
    "print(f\"{train_nodes=}\")\n",
    "train_nodes, val_nodes = get_graph_node_names(ff_model)\n",
    "print(f\"{train_nodes=}\")\n",
    "train_nodes, val_nodes = get_graph_node_names(pc_model)\n",
    "print(f\"{train_nodes=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: it's not returning 5 samples per class\n",
    "import train.ff\n",
    "\n",
    "imgs, targets = sample_images(test_loader, n=5, plot=True)\n",
    "imgs_overlay = train.ff.overlay_label(imgs, targets, n_classes, is_positive=True)\n",
    "\n",
    "# NOTE: analyze after activation (captures full non-linear representation)\n",
    "print(\"Extracting features (backprop)\")\n",
    "layer_names = [\"pool1\", \"pool2\", \"relu_2\", \"relu_3\", \"fc2\"]\n",
    "bp_features = extract_features(bp_model, imgs.to(device), return_layers=layer_names)\n",
    "\n",
    "print(\"\\nExtracting features (forward-forward)\")\n",
    "layer_names = [\"pool1\", \"pool2\", \"conv3.relu\", \"fc1.relu\", \"fc2.linear\"]\n",
    "ff_features = extract_features(\n",
    "    ff_model, imgs_overlay.to(device), return_layers=layer_names\n",
    ")\n",
    "\n",
    "print(\"\\nExtracting features (predictive coding)\")\n",
    "layer_names = [\"0.2\", \"1.2\", \"2.2\", \"3.1\", \"4.0\"]\n",
    "pc_features = extract_features(pc_model, imgs.to(device), return_layers=layer_names)\n",
    "\n",
    "# Rename to match the backpropagation model\n",
    "ff_features = {\n",
    "    \"input\": ff_features[\"input\"],\n",
    "    \"pool1\": ff_features[\"pool1\"],\n",
    "    \"pool2\": ff_features[\"pool2\"],\n",
    "    \"conv3.relu\": ff_features[\"conv3.relu\"],\n",
    "    \"fc1.relu\": ff_features[\"fc1.relu\"],\n",
    "    \"fc2.linear\": ff_features[\"fc2.linear\"],\n",
    "}\n",
    "\n",
    "pc_features = {\n",
    "    \"input\": pc_features[\"input\"],\n",
    "    \"pool1\": pc_features[\"0.2\"],\n",
    "    \"pool2\": pc_features[\"1.2\"],\n",
    "    \"relu_2\": pc_features[\"2.2\"],\n",
    "    \"relu_3\": pc_features[\"3.1\"],\n",
    "    \"fc2\": pc_features[\"4.0\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69481f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdms_bp, rdms_dict_bp = calc_rdms(bp_features)\n",
    "plot_maps(rdms_dict_bp, \"Backpropagation\")\n",
    "\n",
    "rdms_ff, rdms_dict_ff = calc_rdms(ff_features)\n",
    "plot_maps(rdms_dict_ff, \"Forward-forward\")\n",
    "\n",
    "rdms_pc, rdms_dict_pc = calc_rdms(pc_features)\n",
    "plot_maps(rdms_dict_pc, \"Predictive Coding\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
